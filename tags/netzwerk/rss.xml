<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Netzwerk on Michael im Netz</title>
    <link>https://0rph3us.github.io/tags/netzwerk/</link>
    <description>Recent content in Netzwerk on Michael im Netz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>de-DE</language>
    <copyright>Diese Seite ist unter der &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt; lizenziert.</copyright>
    <lastBuildDate>Tue, 28 May 2013 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://0rph3us.github.io/tags/netzwerk/rss/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Hadoop Cluster und das Netzwerk</title>
      <link>https://0rph3us.github.io/post/2013/05/hadoop-cluster-und-das-netzwerk/</link>
      <pubDate>Tue, 28 May 2013 00:00:00 +0000</pubDate>
      
      <guid>https://0rph3us.github.io/post/2013/05/hadoop-cluster-und-das-netzwerk/</guid>
      <description>&lt;p&gt;Ich habe mich heute wieder mit der Architektur von Hadoop-Clustern beschäftigt. Der Softwarestack ist relativ unspektakulär: Linux -&amp;gt; Java -&amp;gt; &lt;a href=&#34;http://hadoop.apache.org/&#34;&gt;Hadoop&lt;/a&gt;.
Beim Hardware-Stack scheiden sich etwas die Geister. Ich habe immer noch mit dem Gerücht zu kämpfen, dass man für Hadoop &lt;em&gt;Schrott-Rechner&lt;/em&gt;
verwenden kann. Hier wird der Begriff &lt;em&gt;Commodity Hardware&lt;/em&gt; etwas falsch interpretiert. Commodity Hardware bezeichnet im
Hadoop-Kontext keine spezielle Hardware verwendet wird. Große Datenbankensysteme verwenden in der Regel sehr spezielle Hardware.&lt;/p&gt;

&lt;p&gt;Ich beobachte einen Trend, dass es immer mehr &lt;a href=&#34;http://de.wikipedia.org/wiki/Appliance&#34;&gt;Appliances&lt;/a&gt; gibt, welche schon recht spezielle Netzwerktechnik verwenden,
welche man ehr im klassischen HPC mit MPI vermuten würde. Wenn man sich die folgende Frage stellt, dann kommt man schnell
selbst zu der Erkenntnis, dass man auch im Hadoop-Umfeld sehr spezielle Hardware benötigt.&lt;/p&gt;

&lt;p&gt;Ich möchte das ganze einmal an einem Beispiel vorführen: Wenn man in einem Hadoop-Knoten 12 3TB große SAS Platten verbaut,
dann ist es nicht unrealistisch, dass man 120 MB/s von jeder Platte lesen bzw. 100 MB/s schreiben kann und das über einen
längeren Zeitraum. Daraus resultiert eine gesamte Bandbreite von 1440 MB/s bzw. 1200 MB/s. Wenn man sich diese Zahlen ansieht,
dann ergibt es durchaus Sinn 2 10 GBit-Interfaces pro Node zu haben. Wenn man von komprimierten Daten im hdfs ausgeht,
welche unkomprimiert versendet werden, dann können auch 2 QDR &lt;a href=&#34;http://de.wikipedia.org/wiki/InfiniBand&#34;&gt;Infiniband&lt;/a&gt;-Interfaces (40 GBit/s) Sinn. Es gibt durchaus Anbieter,
welche auf Infiniband setzten.&lt;/p&gt;

&lt;p&gt;Durch die Administration und den Ausbau das Hadoop-Clusters meines Arbeitgebers kann ich bestätigen, dass die Daten nicht so lokal
bleiben, wie man es sich das wünschen würde. Das ganze kann sich verschärfen, wenn man im
Cluster noch eine Datenbank, wie &lt;a href=&#34;http://hypertable.com/&#34;&gt;Hypertable&lt;/a&gt; betreibt.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cloud Computing</title>
      <link>https://0rph3us.github.io/post/2011/02/cloud-computing/</link>
      <pubDate>Mon, 21 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>https://0rph3us.github.io/post/2011/02/cloud-computing/</guid>
      <description>&lt;p&gt;Ich höre immer öfter von Cloud Computing. Jedes mal hört sich das ganze wie eine ganz neue Idee an. Dabei ist Cloud Computing, nach meiner Ansicht, ein alter Hut. Dieser hat bestimmt schon 50 Jahre auf dem Buckel. Was man unter &lt;a href=&#34;http://de.wikipedia.org/wiki/Cloud_Computing&#34;&gt;Cloud Computing&lt;/a&gt; versteht kann nachlesen. Was ist aber der eigentliche Kern hinter der ganzen Geschichte?&lt;/p&gt;
&lt;p&gt;Beim   Cloud Computing werden Rechen-, Speicherkapazitäten oder Dienste dynamisch zur Verfügung gestellt. Diese Kapazitäten und Dienste werden in der Regel über ein Netzwerk zugänglich gemacht. Auf den Mainframes der 1960er Jahre wurden auch Dienste und Ressourcen dynamisch angeboten und verwaltet. Die Rechenzeit wurde zum Teil auch bei den verschiedenen Kostenstellen gebucht. Auf diese Weise kann man auch aktuelle Enterprise-Server bzw. Mainframes ansetzten und dabei die Hardware dynamisch partitionieren.  Wo ist nun der Unterschied, ob man 4 volle Racks hat oder nur einen großen Rechner (1960 oder heute)?&lt;/p&gt;
&lt;p&gt;Der Unterschied zu 1960 ist, das man seit einigen Jahren vorkonfiguriertes Blech mit entsprechenden Diensten kaufen kann. Ich habe den Vorteil, das ich auf das Blech und dem Dienst Garantie bekommen kann. Weiterhin gibt es Dienstleister, welche einen Dienste anbieten. Als Kunde muss man sich nicht mehr mit der Hardware belasten. Es ist aber durchaus interessant  im eigenen Rechenzentrum eine private Cloud zu betreiben. So kann man schnell auf sich änderte Bedingungen  reagieren.&lt;/p&gt;
&lt;p&gt;Wenn ich es genau nehme, dann betreibe zu Hause meine eigene Cloud. Das Herzstück ist &lt;strong&gt;walhalla&lt;/strong&gt;, auf ihn läuft ein Solaris 11 Express Edition. In dem Rechner sind einige Festplatten. Wenn ich etwas ausprobiere, dann erstelle ich schnell eine  Zone oder setzte mit VirtualBox ein Linux auf. Je nach dem was ich mache, erzeuge ich mir eine maßgeschneiderte Umgebung. Einige werden sich fragen, wozu der ganze Aufwand. Die Antwort ist ganz einfach: Es ist möglich und es macht mir Spaß. Den größten Mehrgewinn sehe ich darin, das ich mein System sauber halte. Denn ich weiß für was welche Zone gut ist.&lt;/p&gt;
&lt;p&gt;P.S.: Ich habe 1960 gewählt, weil mir nichts besseres eingefallen ist.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>svn und Proxy-Server</title>
      <link>https://0rph3us.github.io/post/2010/04/svn-und-proxy-server/</link>
      <pubDate>Mon, 12 Apr 2010 00:00:00 +0000</pubDate>
      
      <guid>https://0rph3us.github.io/post/2010/04/svn-und-proxy-server/</guid>
      <description>&lt;p&gt;Ich sitze dummerweise hinter einem Proxyserver. Als ich ein Repository auschecken wollte hat sich sehr lange nichts getan. Bis ich auf die zündete Idee gekommen bin, dass es vielleicht am Proxy liegt. Einen Proxyserver kann man global in der Datei &lt;tt&gt;/etc/subversion/servers&lt;/tt&gt; oder im Home &lt;tt&gt;.subversion&lt;/tt&gt; konfigurieren&lt;/p&gt;
&lt;pre lang=&#34;bash&#34;&gt;
[global]
http-proxy-host=proxyhost
http-proxy-port=3128
&lt;/pre&gt;
&lt;p&gt;Bei den meisten Linux-Distributionen ist das nicht n&amp;ouml;tig, aber bei (Open) Solaris&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenSolaris auf einer V880 installieren</title>
      <link>https://0rph3us.github.io/post/2010/02/opensolaris-auf-einer-v880-installieren/</link>
      <pubDate>Wed, 24 Feb 2010 00:00:00 +0000</pubDate>
      
      <guid>https://0rph3us.github.io/post/2010/02/opensolaris-auf-einer-v880-installieren/</guid>
      <description>&lt;p&gt;Ich habe heute OpenSolaris auf einer &lt;a href=&#34;http://sunsolve.sun.com/handbook_pub/validateUser.do?target=Systems/SunFire880/SunFire880&#34;&gt;SunFire V880&lt;/a&gt; installiert. Das ganze war nicht ganz so trivial wie ich mir das gedacht habe. Ich habe dazu den &lt;a href=&#34;http://hub.opensolaris.org/bin/view/Project+caiman/TextInstallerProject&#34;&gt;Textinstaller&lt;/a&gt; benutzt. Diese basiert auf Build 131. Man kann sich das Image z.B. von &lt;a href=&#34;http://www.genunix.org/&#34;&gt;genunix.org&lt;/a&gt; herunter laden.&lt;br /&gt;
Hier war mein erster Fehler, ich hatte nich geprüft ob die Checksumme stimmt. Aber irgendwie wollte die Installation auch bei dem 3. Versuch nicht klappen. Die rettende Idee, war einmal die Checksumme vom Image zu bestimmen. Nun habe ich gesehen, dass diese nicht gestimmt hat.&lt;br /&gt;
Nachdem ich ein ganze Image auf eine DVD gebrannt ließ sich OpenSolaris auch installieren. Das Netzwerk kann man zur Zeit nur via DHCP konfigurieren, was ich nicht wollte. Nachdem die Installation fertig war, habe ich mit &lt;a href=&#34;https://blog.binfalse.de/&#34;&gt;Martin&lt;/a&gt; das Netzwerk konfiguriert.   Das macht man wie folgt:&lt;/p&gt;
&lt;pre lang=&#34;bash&#34;&gt;jack@dijkstra ~ $  cat /etc/nwam/llp
eri0 dhcp
ge0 static 192.168.1.100/25
&lt;/pre&gt;
&lt;p&gt;Die default-Route setzt man in der /etc/defaultrouter:&lt;/p&gt;
&lt;pre lang=&#34;bash&#34;&gt;
jack@dijkstra ~ $  cat /etc/defaultrouter
192.168.1.126
&lt;/pre&gt;
&lt;p&gt;Als nächstes stand das spiegeln des rpool auf den Plan. Dazu habe ich mit format -e der 2. Platte ein SMI-Label verpasst und die ganze Platte in Slice 0 gepackt.&lt;/p&gt;
&lt;pre lang=&#34;bash&#34;&gt;jack@dijkstra ~ $ pfexec zpool attach -f rpool c1t0d0s0 c1t1d0s0&lt;/pre&gt;
&lt;p&gt;Abschließend muss man noch die Platte bootbar machen:&lt;/p&gt;
&lt;pre lang=&#34;bash&#34;&gt;jack@dijkstra ~ $ pfexec installboot -F zfs /usr/platform/`uname -i`/lib/fs/zfs/bootblk /dev/rdsk/c1t1d0s0&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>