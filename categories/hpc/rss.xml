<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hpc on Michael im Netz</title>
    <link>http://localhost:1313/categories/hpc/</link>
    <description>Recent content in Hpc on Michael im Netz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>de-DE</language>
    <copyright>Diese Seite ist unter der &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt; lizenziert.</copyright>
    <lastBuildDate>Tue, 28 May 2013 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/hpc/rss/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Hadoop Cluster und das Netzwerk</title>
      <link>http://localhost:1313/post/2013/05/hadoop-cluster-und-das-netzwerk/</link>
      <pubDate>Tue, 28 May 2013 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/post/2013/05/hadoop-cluster-und-das-netzwerk/</guid>
      <description>&lt;p&gt;Ich habe mich heute wieder mit der Architektur von Hadoop-Clustern beschäftigt. Der Softwarestack ist relativ unspektakulär: Linux -&amp;gt; Java -&amp;gt; &lt;a href=&#34;http://hadoop.apache.org/&#34;&gt;Hadoop&lt;/a&gt;.
Beim Hardware-Stack scheiden sich etwas die Geister. Ich habe immer noch mit dem Gerücht zu kämpfen, dass man für Hadoop &lt;em&gt;Schrott-Rechner&lt;/em&gt;
verwenden kann. Hier wird der Begriff &lt;em&gt;Commodity Hardware&lt;/em&gt; etwas falsch interpretiert. Commodity Hardware bezeichnet im
Hadoop-Kontext keine spezielle Hardware verwendet wird. Große Datenbankensysteme verwenden in der Regel sehr spezielle Hardware.&lt;/p&gt;

&lt;p&gt;Ich beobachte einen Trend, dass es immer mehr &lt;a href=&#34;http://de.wikipedia.org/wiki/Appliance&#34;&gt;Appliances&lt;/a&gt; gibt, welche schon recht spezielle Netzwerktechnik verwenden,
welche man ehr im klassischen HPC mit MPI vermuten würde. Wenn man sich die folgende Frage stellt, dann kommt man schnell
selbst zu der Erkenntnis, dass man auch im Hadoop-Umfeld sehr spezielle Hardware benötigt.&lt;/p&gt;

&lt;p&gt;Ich möchte das ganze einmal an einem Beispiel vorführen: Wenn man in einem Hadoop-Knoten 12 3TB große SAS Platten verbaut,
dann ist es nicht unrealistisch, dass man 120 MB/s von jeder Platte lesen bzw. 100 MB/s schreiben kann und das über einen
längeren Zeitraum. Daraus resultiert eine gesamte Bandbreite von 1440 MB/s bzw. 1200 MB/s. Wenn man sich diese Zahlen ansieht,
dann ergibt es durchaus Sinn 2 10 GBit-Interfaces pro Node zu haben. Wenn man von komprimierten Daten im hdfs ausgeht,
welche unkomprimiert versendet werden, dann können auch 2 QDR &lt;a href=&#34;http://de.wikipedia.org/wiki/InfiniBand&#34;&gt;Infiniband&lt;/a&gt;-Interfaces (40 GBit/s) Sinn. Es gibt durchaus Anbieter,
welche auf Infiniband setzten.&lt;/p&gt;

&lt;p&gt;Durch die Administration und den Ausbau das Hadoop-Clusters meines Arbeitgebers kann ich bestätigen, dass die Daten nicht so lokal
bleiben, wie man es sich das wünschen würde. Das ganze kann sich verschärfen, wenn man im
Cluster noch eine Datenbank, wie &lt;a href=&#34;http://hypertable.com/&#34;&gt;Hypertable&lt;/a&gt; betreibt.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cloud Computing</title>
      <link>http://localhost:1313/post/2011/02/cloud-computing/</link>
      <pubDate>Mon, 21 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/post/2011/02/cloud-computing/</guid>
      <description>&lt;p&gt;Ich höre immer öfter von Cloud Computing. Jedes mal hört sich das ganze wie eine ganz neue Idee an. Dabei ist Cloud Computing, nach meiner Ansicht, ein alter Hut. Dieser hat bestimmt schon 50 Jahre auf dem Buckel. Was man unter &lt;a href=&#34;http://de.wikipedia.org/wiki/Cloud_Computing&#34;&gt;Cloud Computing&lt;/a&gt; versteht kann nachlesen. Was ist aber der eigentliche Kern hinter der ganzen Geschichte?&lt;/p&gt;
&lt;p&gt;Beim   Cloud Computing werden Rechen-, Speicherkapazitäten oder Dienste dynamisch zur Verfügung gestellt. Diese Kapazitäten und Dienste werden in der Regel über ein Netzwerk zugänglich gemacht. Auf den Mainframes der 1960er Jahre wurden auch Dienste und Ressourcen dynamisch angeboten und verwaltet. Die Rechenzeit wurde zum Teil auch bei den verschiedenen Kostenstellen gebucht. Auf diese Weise kann man auch aktuelle Enterprise-Server bzw. Mainframes ansetzten und dabei die Hardware dynamisch partitionieren.  Wo ist nun der Unterschied, ob man 4 volle Racks hat oder nur einen großen Rechner (1960 oder heute)?&lt;/p&gt;
&lt;p&gt;Der Unterschied zu 1960 ist, das man seit einigen Jahren vorkonfiguriertes Blech mit entsprechenden Diensten kaufen kann. Ich habe den Vorteil, das ich auf das Blech und dem Dienst Garantie bekommen kann. Weiterhin gibt es Dienstleister, welche einen Dienste anbieten. Als Kunde muss man sich nicht mehr mit der Hardware belasten. Es ist aber durchaus interessant  im eigenen Rechenzentrum eine private Cloud zu betreiben. So kann man schnell auf sich änderte Bedingungen  reagieren.&lt;/p&gt;
&lt;p&gt;Wenn ich es genau nehme, dann betreibe zu Hause meine eigene Cloud. Das Herzstück ist &lt;strong&gt;walhalla&lt;/strong&gt;, auf ihn läuft ein Solaris 11 Express Edition. In dem Rechner sind einige Festplatten. Wenn ich etwas ausprobiere, dann erstelle ich schnell eine  Zone oder setzte mit VirtualBox ein Linux auf. Je nach dem was ich mache, erzeuge ich mir eine maßgeschneiderte Umgebung. Einige werden sich fragen, wozu der ganze Aufwand. Die Antwort ist ganz einfach: Es ist möglich und es macht mir Spaß. Den größten Mehrgewinn sehe ich darin, das ich mein System sauber halte. Denn ich weiß für was welche Zone gut ist.&lt;/p&gt;
&lt;p&gt;P.S.: Ich habe 1960 gewählt, weil mir nichts besseres eingefallen ist.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mehrkern oder nicht Mehrkern, dass ist die Frage</title>
      <link>http://localhost:1313/post/2010/06/mehrkern-oder-nicht-mehrkern-dass-ist-die-frage/</link>
      <pubDate>Mon, 21 Jun 2010 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/post/2010/06/mehrkern-oder-nicht-mehrkern-dass-ist-die-frage/</guid>
      <description>&lt;p&gt;Ich habe inzwischen ein paar Experimente mit verschiedenen Mehrkernarchitekturen gemacht. Ich habe verschiende Algorithmen mit &lt;a href=&#34;http://en.wikipedia.org/wiki/Cilk&#34;&gt;Cilk++&lt;/a&gt; implementiert. Dabei kam es zu interessanten Ergebnissen. Ich hatte von superlinearen Speedup bis zu einem Speedup unter 1 alles. Aber woran liegt das, dass die Ergebnisse so weit auseinander gehen? Die schlechten Ergebnisse habe ich auf einem 2 Sockel &lt;a href=&#34;http://ark.intel.com/Product.aspx?id=33927&#34;&gt;Intel Xeon E5420&lt;/a&gt;-System mit 32 GB RAM gemacht. In der Mitte lag ein 2 Sockel System mit &lt;a href=&#34;http://ark.intel.com/Product.aspx?id=37111&#34;&gt;Intel Xeon X5570&lt;/a&gt; und 48 GB RAM Die besten Ergebnisse lieferte ein 4 Sockel Rechner mit 16 GB RAM und &lt;a href=&#34;http://www.cpu-world.com/CPUs/K8/AMD-Opteron%20852%20-%20OSP852FAA5BM.html&#34;&gt;AMD Opteron 852&lt;/a&gt; Prozessoren.&lt;/p&gt;
&lt;p&gt;
Eine Erklärung für das unterschiedliche abschneiden der Systeme ist die Anbindung an den RAM und die Caches. Bei dem AMD-System hat jeder (Einkern-) Prozessor privaten Cache und einen eignen Speicherkontroller. Hier gibt es keine Engpässe. Bei dem &lt;a href=&#34;http://ark.intel.com/Product.aspx?id=37111&#34;&gt;Intel Xeon X5570&lt;/a&gt; sind die Speicherkontroller in der CPU, aber der L2 Cache ist shared zwischen 2 Kernen. &lt;a href=&#34;http://www.intel.com/technology/platform-technology/hyper-threading/&#34;&gt;Hyperthreading&lt;/a&gt; hat keine Verbesserung der Laufzeit gebacht. Am schlechtesten schnitt das &lt;a href=&#34;http://ark.intel.com/Product.aspx?id=33927&#34;&gt;Intel Xeon E5420&lt;/a&gt;-System ab. Die beiden Prozessoren gehen über die Nordbrücke, um an den RAM zu gelangen. Ab einer bestimmten Problemgröße wurden die Algorithmen über die Speicherzugriffe sequenzialisiert.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dein Freund der Cachemiss</title>
      <link>http://localhost:1313/post/2010/05/dein-freund-der-cachemiss/</link>
      <pubDate>Fri, 14 May 2010 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/post/2010/05/dein-freund-der-cachemiss/</guid>
      <description>&lt;p&gt;
Ich habe in meinen &lt;a href=&#34;http://0rpheus.net/hpc/superlinearer-speedup&#34;&gt;letzten Eintrag &amp;uuml;ber Superlinearen Speedup&lt;/a&gt; geschrieben. Caching Effekte lassen sich auch in sequenziellen Programmen ausnutzen. So kann man l&amp;auml;sst sich die klassische Matrixmultiplikation um Gr&amp;ouml;&amp;szlig;enordnungen beschleunigen.&lt;/p&gt;
&lt;p&gt;
Dazu muss man nur die Matrix B transponiert abspeichern. Wenn man jetzt duch die Spalten der Matrix B geht hat man eine h&amp;ouml;here Lokalit&amp;auml;t und damit weniger Cachemisses.&lt;/p&gt;
&lt;pre lang=&#34;cpp&#34;&gt;
#include &lt;iostream&gt;
#include &lt;stdlib.h&gt;
#include &lt;time.h&gt;

using namespace std;

void mul(double *A, double *B, double *C, int dim)
{
    int i, j, k;
    double s;

    for (i = 0; i &lt; dim; i++){
        for (j = 0; j &lt; dim; j++) {
            s = 0.0;
            for (k = 0; k &lt; dim; ++k)
                s += A[dim*i +k] * B[dim*k+j];

            C[dim*i+j] = s;
        }
    }
}

void transpose(double *A, int dim){

        int i,j;
        double tmp;

    for (i = 0; i &lt; dim; i++){
        for (j = i; j &lt; dim; j++){
            tmp = A[dim*i+j];
            A[dim*i+j] =A[dim*j+i];
            A[dim*j+i] = tmp;
        }
    }

}

void mulfast(double *A, double *B, double *C, int dim)
{
    int i, j, k;
    double s;

    transpose(B, dim);

    for (i = 0; i &lt; dim; ++i){
        for (j = 0; j &lt; dim; ++j) {
            s = 0.0;
            for (k = 0; k &lt; dim; ++k)
                // B is transposed !!
                s += A[dim*i +k] * B[dim*j+k];

            C[dim*i+j] = s;
        }
    }

    transpose(B, dim);
}

int main(){

    int dim = 1024;
    clock_t start, end;

    double* A = (double*) calloc(dim* dim, sizeof(double));
    double* B = (double*) calloc(dim* dim, sizeof(double));
    double* C = (double*) calloc(dim* dim, sizeof(double));

    start = clock();
    mul(A,B,C, dim);
    end = clock();
    cout &lt;&lt; &#34;normal implementation: &#34; &lt;&lt; (end-start) / CLOCKS_PER_SEC &lt;&lt; &#34; seconds&#34; &lt;&lt; endl;

    start = clock();
    mulfast(A,B,C, dim);
    end = clock();
    cout &lt;&lt; &#34;fast implementation: &#34; &lt;&lt; (end-start) / CLOCKS_PER_SEC &lt;&lt; &#34; seconds&#34; &lt;&lt; endl;

    return 0;
}
&lt;/time.h&gt;&lt;/stdlib.h&gt;&lt;/iostream&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Superlinearer Speedup</title>
      <link>http://localhost:1313/post/2010/05/superlinearer-speedup/</link>
      <pubDate>Fri, 14 May 2010 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/post/2010/05/superlinearer-speedup/</guid>
      <description>&lt;p&gt;Ich habe für meine Diplomarbeit die &lt;a href=&#34;http://de.wikipedia.org/wiki/Strassen-Algorithmus&#34;&gt;Matrixmultiplikation nach Strassen&lt;/a&gt; implementiert. Das ganze habe ich mit
&lt;a href=&#34;http://en.wikipedia.org/wiki/Cilk&#34;&gt;Cilk++&lt;/a&gt; implementiert.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://localhost:1313/strassen-results.png&#34; title=&#34;&#34; data-lightbox=&#34;set1&#34; data-title=&#34;Speedup von Matrixmultiplikation nach Strassen&#34;&gt;&lt;img src=&#34;http://localhost:1313/strassen-results-thumbnail.png&#34; alt=&#34;superlinearer Speedup&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ich war recht erstaunt, als ich diese Ergebnisse gesehen habe. Es ist erklärbar, weswegen ich einen superlinearen
&lt;a href=&#34;http://de.wikipedia.org/wiki/Speedup&#34;&gt;Speedup&lt;/a&gt; erreicht habe. Wenn ich auf 4 CPUs rechne habe ich mehr &lt;a href=&#34;http://de.wikipedia.org/wiki/Cache&#34;&gt;Cache&lt;/a&gt; zur Verfügung. Diesen hohen
Speedup kann man nur durch Caching Effekte erreichen.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>